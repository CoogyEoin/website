<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Web Scraping Energy Data with Python and AWS | Eoin&#39;s Website</title>
<meta name="keywords" content="">
<meta name="description" content="Building an app on AWS to help save energy bills and the environment
The idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it.">
<meta name="author" content="">
<link rel="canonical" href="http://eoincoogan.com/posts/electriciy_web_scraper/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css" integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://eoincoogan.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://eoincoogan.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://eoincoogan.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://eoincoogan.com/apple-touch-icon.png">
<link rel="mask-icon" href="http://eoincoogan.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Web Scraping Energy Data with Python and AWS" />
<meta property="og:description" content="Building an app on AWS to help save energy bills and the environment
The idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://eoincoogan.com/posts/electriciy_web_scraper/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-01T15:57:17+00:00" />
<meta property="article:modified_time" content="2022-11-01T15:57:17+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Web Scraping Energy Data with Python and AWS"/>
<meta name="twitter:description" content="Building an app on AWS to help save energy bills and the environment
The idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://eoincoogan.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Web Scraping Energy Data with Python and AWS",
      "item": "http://eoincoogan.com/posts/electriciy_web_scraper/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Web Scraping Energy Data with Python and AWS",
  "name": "Web Scraping Energy Data with Python and AWS",
  "description": "Building an app on AWS to help save energy bills and the environment\nThe idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it.",
  "keywords": [
    
  ],
  "articleBody": "Building an app on AWS to help save energy bills and the environment\nThe idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it.\nI haven’t worked on the app itself but to summarize the back-end will gather real-time electricity prices and notify users when the price rises in their location. This allows people who use time-of-use utility plans to reduce their bills and help offload the demand on the grid during peak times. The backend of the mobile app (all the stuff the user doesn’t see) will require a web-scraping script to gather this price data from the various ISO websites. This article will discuss how I did that using only python and some AWS services.\nWeb-scraping is just the process of gathering data from a website using a software program. Although using APIs are desirable it’s not uncommon to find that some web services don’t provide them (or only allow stakeholders to access them). This means we need to find alternative ways to get this data. The scripts themselves are relatively simple. Once they are written though we need a way to run them automatically and in a scalable way. This is where using AWS services comes in handy. Requirements\nScripts that will gather the price data from ISO websites. Store the price data in a database for further use. Automate the scripts so they run every 5 minutes. The script I wrote uses the requests python library to make a get request to the ISO website. The response contains the typical HTML code that can be seen using inspect elements on a webpage. Another library called Beautiful Soup is used to prettify this HTML code and allows elements to be found easily. The code for one of the ISOs can be seen below.\nimport requests import logging from bs4 import BeautifulSoup dict = { \"DPL\":\"\", \"COMED\":\"\", \"AEP\":\"\", \"EKPC\":\"\", \"PEP\":\"\", \"JC\":\"\", \"PL\":\"\", \"DOM\":\"\" } def main(): try: URL = 'https://www.pjm.com/' page = requests.get(URL) soup = BeautifulSoup(page.content, 'html.parser') \"\"\" There are multiple tables of class 'lmp-price-table'. This loop goes through all the divs in the tables containing the RSO name and the LMP and if the key exists in the dictionary (Keys are the RSO name) then it stores the next divs text as the value \"\"\" for i in soup.find_all('ul', class_='lmp-price-table'): divs = i.findChildren('div') j=0 while j \u003c len(divs): value = divs[j].text if value in lmp_dict: lmp_dict[value] = divs[j+1].text j+=1 print(lmp_dict) except Exception as e: raise e if __name__ == \"__main__\": main() A script was created for a few different ISOs which each have different websites. The scripts can be viewed in my GitHub repo below.\nGithub Repo\nOnce the scripts worked they needed to be hosted on AWS. Lambda is a service specifically for simple scripts that allows you to invoke them from other services or on a schedule with CloudWatch. More information can be found here. We also needed to store the price data in a database. I decided to use DynamoDB which is a NoSQL DB because it is much cheaper for read operations (we’ll only be writing new data every 5 minutes) and if the schema needs to change in the future there won’t be any major issues. So the Lambda function needed to have some additional handlers so that it could be ran and an additional function for putting the data to DynamoDB.\nOnce the scripts were working and storing data in the DynamoDB table the only thing left to do is to automate it. CloudWatch is a service which is really handy for collecting logs from events. Lambda functions by default publish logs using CloudWatch whenever it runs. Under the events section of the CloudWatch dashboard there is an option to create a schedule. I set the schedule for 5 minutes and the target as my two lambda scripts. This means we don’t have to worry about any issues with running a fail/success events can be monitored if you need to know about them (such as sending an email to you on failure to run).\nThere is a lot more to do with this project but for the time being, that’s everything to do with the data collection.\n",
  "wordCount" : "746",
  "inLanguage": "en",
  "datePublished": "2022-11-01T15:57:17Z",
  "dateModified": "2022-11-01T15:57:17Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://eoincoogan.com/posts/electriciy_web_scraper/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Eoin's Website",
    "logo": {
      "@type": "ImageObject",
      "url": "http://eoincoogan.com/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://eoincoogan.com/" accesskey="h" title="Eoin&#39;s Website (Alt + H)">Eoin&#39;s Website</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Web Scraping Energy Data with Python and AWS
    </h1>
    <div class="post-meta"><span title='2022-11-01 15:57:17 +0000 UTC'>November 1, 2022</span>

</div>
  </header> 
  <div class="post-content"><p>Building an app on AWS to help save energy bills and the environment</p>
<p>The idea behind this article came to me a while back. While writing a previous article about predicting energy demand I thought of the idea to build an app that would notify users when electricity prices are high. Originally this was an addition to the previous application but I quickly realized that regular people would get more benefit out of it.</p>
<p>I haven&rsquo;t worked on the app itself but to summarize the back-end will gather real-time electricity prices and notify users when the price rises in their location. This allows people who use time-of-use utility plans to reduce their bills and help offload the demand on the grid during peak times. The backend of the mobile app (all the stuff the user doesn’t see) will require a web-scraping script to gather this price data from the various ISO websites. This article will discuss how I did that using only python and some AWS services.</p>
<p>Web-scraping is just the process of gathering data from a website using a software program. Although using APIs are desirable it’s not uncommon to find that some web services don’t provide them (or only allow stakeholders to access them). This means we need to find alternative ways to get this data. The scripts themselves are relatively simple. Once they are written though we need a way to run them automatically and in a scalable way. This is where using AWS services comes in handy.
Requirements</p>
<ul>
<li>Scripts that will gather the price data from ISO websites.</li>
<li>Store the price data in a database for further use.</li>
<li>Automate the scripts so they run every 5 minutes.</li>
</ul>
<p>The script I wrote uses the requests python library to make a get request to the ISO website. The response contains the typical HTML code that can be seen using inspect elements on a webpage. Another library called Beautiful Soup is used to prettify this HTML code and allows elements to be found easily. The code for one of the ISOs can be seen below.</p>
<pre><code>import requests
import logging
from bs4 import BeautifulSoup
dict = {
    &quot;DPL&quot;:&quot;&quot;,
    &quot;COMED&quot;:&quot;&quot;,
    &quot;AEP&quot;:&quot;&quot;,
    &quot;EKPC&quot;:&quot;&quot;,
    &quot;PEP&quot;:&quot;&quot;,
    &quot;JC&quot;:&quot;&quot;,
    &quot;PL&quot;:&quot;&quot;,
    &quot;DOM&quot;:&quot;&quot;
}
def main():
	    try:
            URL = 'https://www.pjm.com/'
    	page = requests.get(URL)
    	soup = BeautifulSoup(page.content, 'html.parser')

    	&quot;&quot;&quot;
    	There are multiple tables of class 'lmp-price-table'. This
    	loop goes through all the divs in the tables containing the RSO name 
    	and the LMP and if the key exists in the dictionary (Keys are the RSO 
    	name) then it stores the next divs text as the value
    	&quot;&quot;&quot;
    	for i in soup.find_all('ul', class_='lmp-price-table'):
        	    divs = i.findChildren('div')
        	    j=0
        	    while j &lt; len(divs):
            	value = divs[j].text
            	if value in lmp_dict:
                	    lmp_dict[value] = divs[j+1].text
            	j+=1
        
    	    print(lmp_dict)


		except Exception as e:
    	raise e

	if __name__ == &quot;__main__&quot;:
	    	    main()
</code></pre>
<p>A script was created for a few different ISOs which each have different websites. The scripts can be viewed in my GitHub repo below.</p>
<p><a href="https://github.com/CoogyEoin/grianity_price_scripts">Github Repo</a></p>
<p>Once the scripts worked they needed to be hosted on AWS. Lambda is a service specifically for simple scripts that allows you to invoke them from other services or on a schedule with CloudWatch. More information can be found here. We also needed to store the price data in a database. I decided to use DynamoDB which is a NoSQL DB because it is much cheaper for read operations (we’ll only be writing new data every 5 minutes) and if the schema needs to change in the future there won’t be any major issues. So the Lambda function needed to have some additional handlers so that it could be ran and an additional function for putting the data to DynamoDB.</p>
<p>Once the scripts were working and storing data in the DynamoDB table the only thing left to do is to automate it. CloudWatch is a service which is really handy for collecting logs from events. Lambda functions by default publish logs using CloudWatch whenever it runs. Under the events section of the CloudWatch dashboard there is an option to create a schedule. I set the schedule for 5 minutes and the target as my two lambda scripts. This means we don’t have to worry about any issues with running a fail/success events can be monitored if you need to know about them (such as sending an email to you on failure to run).</p>
<p>There is a lot more to do with this project but for the time being, that’s everything to do with the data collection.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="http://eoincoogan.com/">Eoin&#39;s Website</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
